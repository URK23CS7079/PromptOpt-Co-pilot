# PromptOpt Co-Pilot - Llama Wrapper Dependencies
# Production requirements for the llama.cpp wrapper

# Core LLM inference
llama-cpp-python>=0.2.11
# Note: For GPU support, install with: pip install llama-cpp-python[gpu]

# Web API framework
fastapi>=0.104.1
uvicorn[standard]>=0.24.0

# Data validation and serialization
pydantic>=2.4.2

# System monitoring
psutil>=5.9.6

# Async support
asyncio-mqtt>=0.13.0  # For async operations
aiofiles>=23.2.1      # For async file operations

# Development and testing dependencies (optional)
pytest>=7.4.3
pytest-asyncio>=0.21.1
httpx>=0.25.2          # For TestClient
coverage>=7.3.2
black>=23.11.0         # Code formatting
flake8>=6.1.0          # Linting
mypy>=1.7.1            # Type checking

# Logging and monitoring
structlog>=23.2.0      # Structured logging
prometheus-client>=0.19.0  # Metrics collection (optional)

# Configuration management
python-dotenv>=1.0.0   # Environment variable loading
pyyaml>=6.0.1          # YAML config support

# Memory optimization
numpy>=1.24.4          # For efficient array operations
scipy>=1.11.4          # Scientific computing utilities

# Optional GPU acceleration dependencies
# Uncomment if using GPU acceleration:
# torch>=2.1.0
# transformers>=4.35.0

# Development tools (install with: pip install -e ".[dev]")
# pytest-cov>=4.1.0
# pre-commit>=3.5.0
# sphinx>=7.2.6         # Documentation